<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/configs/expert_configs.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/configs/expert_configs.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;专家配置模块&#10;定义各个专家的配置函数，包括数据集、模型、优化器等&#10;&quot;&quot;&quot;&#10;import torch&#10;import torch.nn as nn&#10;from torch.utils.data import Dataset, DataLoader&#10;from torch.optim import AdamW&#10;import numpy as np&#10;import sys&#10;from pathlib import Path&#10;&#10;# 添加项目根目录到路径&#10;sys.path.insert(0, str(Path(__file__).parent.parent))&#10;&#10;from src.dataset import Twibot20&#10;from src.model import DesExpert, DesExpertMoE, TweetsExpert, GraphExpert&#10;&#10;# 获取项目根目录&#10;PROJECT_ROOT = Path(__file__).parent.parent&#10;&#10;&#10;# ==================== 辅助函数 ====================&#10;&#10;def _extract_expert_features_with_mask(expert_name, node_indices, dataset_path, checkpoint_dir, device, twibot_dataset=None):&#10;    &quot;&quot;&quot;&#10;    从已训练的专家模型中提取特征&#10;&#10;    Args:&#10;        expert_name: 专家名称 ('des', 'tweets')&#10;        node_indices: 需要提取特征的节点索引列表&#10;        dataset_path: 数据集路径&#10;        checkpoint_dir: 模型检查点目录&#10;        device: 设备&#10;        twibot_dataset: 预加载的数据集（可选）&#10;&#10;    Returns:&#10;        embeddings: [num_nodes, expert_dim] 特征向量&#10;        mask: [num_nodes] bool tensor，标记哪些节点有有效特征&#10;    &quot;&quot;&quot;&#10;    from tqdm import tqdm&#10;&#10;    # 加载数据集（如果未提供）&#10;    if twibot_dataset is None:&#10;        twibot_dataset = Twibot20(root=dataset_path, device=device, process=True, save=True)&#10;&#10;    # 加载已训练的专家模型&#10;    checkpoint_path = Path(checkpoint_dir) / f'{expert_name}_expert_best.pth'&#10;    if not checkpoint_path.exists():&#10;        raise FileNotFoundError(f&quot;未找到专家模型检查点: {checkpoint_path}&quot;)&#10;&#10;    print(f&quot;    加载模型检查点: {checkpoint_path}&quot;)&#10;    checkpoint = torch.load(checkpoint_path, map_location=device)&#10;&#10;    # 根据专家类型创建模型并加载权重&#10;    if expert_name == 'des':&#10;        model = DesExpert(device=device).to(device)&#10;        model.load_state_dict(checkpoint['model_state_dict'])&#10;&#10;        # 获取描述数据&#10;        descriptions = twibot_dataset.Des_preprocess()&#10;        if isinstance(descriptions, np.ndarray):&#10;            descriptions = descriptions.tolist()&#10;&#10;        # 提取特征&#10;        model.eval()&#10;        embeddings_list = []&#10;        valid_mask = []&#10;&#10;        batch_size = 32&#10;        with torch.no_grad():&#10;            for i in tqdm(range(0, len(node_indices), batch_size), desc=f&quot;  提取 {expert_name} 特征&quot;):&#10;                batch_indices = node_indices[i:i+batch_size]&#10;                batch_descriptions = [descriptions[idx] for idx in batch_indices]&#10;&#10;                # 检查是否有有效描述&#10;                batch_valid = []&#10;                for desc in batch_descriptions:&#10;                    desc_str = str(desc).strip()&#10;                    is_valid = desc_str != '' and desc_str.lower() != 'none'&#10;                    batch_valid.append(is_valid)&#10;&#10;                # 提取特征&#10;                expert_repr, _ = model(batch_descriptions)&#10;                embeddings_list.append(expert_repr.cpu())&#10;                valid_mask.extend(batch_valid)&#10;&#10;        embeddings = torch.cat(embeddings_list, dim=0)&#10;        mask = torch.tensor(valid_mask, dtype=torch.bool)&#10;&#10;    elif expert_name == 'tweets':&#10;        model = TweetsExpert(device=device).to(device)&#10;        model.load_state_dict(checkpoint['model_state_dict'])&#10;&#10;        # 获取推文数据&#10;        tweets_list = twibot_dataset.tweets_preprogress()&#10;        if isinstance(tweets_list, np.ndarray):&#10;            tweets_list = tweets_list.tolist()&#10;&#10;        # 提取特征&#10;        model.eval()&#10;        embeddings_list = []&#10;        valid_mask = []&#10;&#10;        batch_size = 32&#10;        with torch.no_grad():&#10;            for i in tqdm(range(0, len(node_indices), batch_size), desc=f&quot;  提取 {expert_name} 特征&quot;):&#10;                batch_indices = node_indices[i:i+batch_size]&#10;                batch_tweets = [tweets_list[idx] for idx in batch_indices]&#10;&#10;                # 检查是否有有效推文&#10;                batch_valid = []&#10;                for user_tweets in batch_tweets:&#10;                    if isinstance(user_tweets, list) and len(user_tweets) &gt; 0:&#10;                        cleaned = [str(t).strip() for t in user_tweets if str(t).strip() != '' and str(t).strip() != 'None']&#10;                        is_valid = len(cleaned) &gt; 0&#10;                    else:&#10;                        is_valid = False&#10;                    batch_valid.append(is_valid)&#10;&#10;                # 提取特征&#10;                expert_repr, _ = model(batch_tweets)&#10;                embeddings_list.append(expert_repr.cpu())&#10;                valid_mask.extend(batch_valid)&#10;&#10;        embeddings = torch.cat(embeddings_list, dim=0)&#10;        mask = torch.tensor(valid_mask, dtype=torch.bool)&#10;&#10;    else:&#10;        raise ValueError(f&quot;不支持的专家类型: {expert_name}&quot;)&#10;&#10;    print(f&quot;    特征形状: {embeddings.shape}, 有效样本: {mask.sum().item()}/{len(mask)}&quot;)&#10;&#10;    return embeddings, mask&#10;&#10;&#10;# ==================== Description Expert ====================&#10;&#10;class DescriptionDataset(Dataset):&#10;    &quot;&quot;&quot;Description 专家数据集&quot;&quot;&quot;&#10;    def __init__(self, descriptions, labels, mode='train'):&#10;        &quot;&quot;&quot;&#10;        Args:&#10;            descriptions: 简介列表&#10;            labels: 标签列表&#10;            mode: 'train' | 'val' | 'test'&#10;                  所有阶段都过滤掉空简介的样本&#10;        &quot;&quot;&quot;&#10;        self.mode = mode&#10;&#10;        # 所有阶段都过滤掉空简介的样本&#10;        valid_indices = []&#10;        for idx, desc in enumerate(descriptions):&#10;            desc_str = str(desc).strip()&#10;            if desc_str != '' and desc_str.lower() != 'none':&#10;                valid_indices.append(idx)&#10;&#10;        self.descriptions = [descriptions[i] for i in valid_indices]&#10;        self.labels = [labels[i] for i in valid_indices]&#10;&#10;        filtered_count = len(descriptions) - len(self.descriptions)&#10;        print(f&quot;  [{mode}集] 有效样本: {len(self.descriptions)}/{len(descriptions)} (过滤 {filtered_count} 个空简介样本)&quot;)&#10;&#10;    def __len__(self):&#10;        return len(self.descriptions)&#10;&#10;    def __getitem__(self, idx):&#10;        description = str(self.descriptions[idx])&#10;        label = self.labels[idx]&#10;&#10;        return {&#10;            'description_text': description,&#10;            'label': torch.tensor(label, dtype=torch.float32)&#10;        }&#10;&#10;&#10;def create_des_expert_config(&#10;    dataset_path=None,&#10;    batch_size=32,&#10;    learning_rate=5e-4,&#10;    weight_decay=0.01,&#10;    device='cuda',&#10;    checkpoint_dir='../../autodl-fs/checkpoints',&#10;    model_name='microsoft/deberta-v3-base',&#10;    max_grad_norm=1.0,&#10;    dropout=0.3,&#10;    early_stopping_patience=4,&#10;    num_experts=4,  # MoE 中的专家数量（默认4个）&#10;    top_k=2,        # Top-K 选择，每次只用权重最大的K个专家（默认2个）&#10;    twibot_dataset=None&#10;):&#10;    &quot;&quot;&quot;&#10;    创建 Description Expert 配置 (使用 DeBERTa-v3-base + MoE + Top-K)&#10;&#10;    Args:&#10;        model_name: 预训练模型名称，默认为 'microsoft/deberta-v3-base'&#10;        batch_size: 批次大小，默认32&#10;        learning_rate: 学习率，默认5e-4 (0.0005)&#10;        weight_decay: 权重衰减，默认0.01&#10;        dropout: Dropout率，默认0.3&#10;        max_grad_norm: 梯度裁剪阈值，默认1.0&#10;        early_stopping_patience: 早停耐心值，默认4&#10;        num_experts: MoE 中的专家数量，默认4&#10;        top_k: 每次选择的专家数量（Top-K），默认2&#10;        twibot_dataset: 预加载的Twibot20数据集对象（可选，避免重复加载）&#10;&#10;    Returns:&#10;        dict: 包含模型、数据加载器、优化器等的配置字典&#10;    &quot;&quot;&quot;&#10;    if dataset_path is None:&#10;        dataset_path = str(PROJECT_ROOT / 'processed_data')&#10;&#10;    print(f&quot;\n{'='*60}&quot;)&#10;    print(f&quot;配置 Description Expert with MoE + Top-K (DeBERTa-v3-base)&quot;)&#10;    print(f&quot;{'='*60}&quot;)&#10;    print(f&quot;  模型: {model_name}&quot;)&#10;    print(f&quot;  专家数量: {num_experts}&quot;)&#10;    print(f&quot;  Top-K 选择: {top_k} (每次只用权重最大的{top_k}个专家)&quot;)&#10;    print(f&quot;  批次大小: {batch_size}&quot;)&#10;    print(f&quot;  学习率: {learning_rate}&quot;)&#10;    print(f&quot;  权重衰减: {weight_decay}&quot;)&#10;    print(f&quot;  Dropout: {dropout}&quot;)&#10;    print(f&quot;  梯度裁剪: {max_grad_norm}&quot;)&#10;    print(f&quot;  早停耐心值: {early_stopping_patience}&quot;)&#10;&#10;    # 加载数据（如果没有预加载）&#10;    if twibot_dataset is None:&#10;        print(&quot;加载数据...&quot;)&#10;        twibot_dataset = Twibot20(root=dataset_path, device=device, process=True, save=True)&#10;    else:&#10;        print(&quot;使用预加载的数据集...&quot;)&#10;&#10;    descriptions = twibot_dataset.Des_preprocess()&#10;    labels = twibot_dataset.load_labels()&#10;&#10;    if isinstance(descriptions, np.ndarray):&#10;        descriptions = descriptions.tolist()&#10;    labels = labels.cpu().numpy()&#10;&#10;    # 获取训练/验证/测试集索引&#10;    train_idx, val_idx, test_idx = twibot_dataset.train_val_test_mask()&#10;    train_idx = list(train_idx)&#10;    val_idx = list(val_idx)&#10;    test_idx = list(test_idx)&#10;&#10;    # 划分数据集&#10;    train_descriptions = [descriptions[i] for i in train_idx]&#10;    train_labels = labels[train_idx]&#10;&#10;    val_descriptions = [descriptions[i] for i in val_idx]&#10;    val_labels = labels[val_idx]&#10;&#10;    test_descriptions = [descriptions[i] for i in test_idx]&#10;    test_labels = labels[test_idx]&#10;&#10;    print(f&quot;  训练集: {len(train_descriptions)} 样本&quot;)&#10;    print(f&quot;  验证集: {len(val_descriptions)} 样本&quot;)&#10;    print(f&quot;  测试集: {len(test_descriptions)} 样本&quot;)&#10;&#10;    # 创建数据集和数据加载器&#10;    print(&quot;创建数据加载器...&quot;)&#10;    train_dataset = DescriptionDataset(train_descriptions, train_labels, mode='train')&#10;    val_dataset = DescriptionDataset(val_descriptions, val_labels, mode='val')&#10;    test_dataset = DescriptionDataset(test_descriptions, test_labels, mode='test')&#10;&#10;    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)&#10;    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)&#10;    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)&#10;&#10;    # 初始化模型 (使用 MoE + Top-K 版本)&#10;    print(&quot;初始化 MoE + Top-K 模型...&quot;)&#10;    model = DesExpertMoE(&#10;        model_name=model_name,&#10;        device=device,&#10;        dropout=dropout,&#10;        num_experts=num_experts,&#10;        top_k=top_k,&#10;        expert_dim=64,&#10;        hidden_dim=768&#10;    ).to(device)&#10;    print(f&quot;  模型参数数量: {sum(p.numel() for p in model.parameters()):,}&quot;)&#10;    print(f&quot;  可训练参数数量: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}&quot;)&#10;    print(f&quot;  - Gating Network + {num_experts} 个 MLP 专家 (Top-{top_k} 选择)&quot;)&#10;&#10;    # 优化器和损失函数 - 使用权重衰减&#10;    optimizer = AdamW(&#10;        filter(lambda p: p.requires_grad, model.parameters()),&#10;        lr=learning_rate,&#10;        weight_decay=weight_decay,&#10;        betas=(0.9, 0.999),&#10;        eps=1e-8&#10;    )&#10;    criterion = nn.BCELoss()&#10;&#10;    # 数据提取函数&#10;    def extract_fn(batch, device):&#10;        description_texts = batch['description_text']&#10;        labels = batch['label'].to(device).unsqueeze(1)&#10;        return (description_texts,), labels&#10;&#10;    return {&#10;        'name': 'des',&#10;        'model': model,&#10;        'train_loader': train_loader,&#10;        'val_loader': val_loader,&#10;        'test_loader': test_loader,&#10;        'optimizer': optimizer,&#10;        'criterion': criterion,&#10;        'device': device,&#10;        'checkpoint_dir': checkpoint_dir,&#10;        'extract_fn': extract_fn,&#10;        'max_grad_norm': max_grad_norm,  # 梯度裁剪&#10;        'early_stopping_patience': early_stopping_patience,  # 早停&#10;        'batch_size': batch_size,&#10;        'learning_rate': learning_rate,&#10;        'dropout': dropout,&#10;        'num_experts': num_experts,  # MoE 专家数量&#10;        'top_k': top_k  # Top-K 选择&#10;    }&#10;&#10;&#10;# ==================== Tweets Expert ====================&#10;&#10;class TweetsDataset(Dataset):&#10;    def __init__(self, tweets_list, labels, mode='train'):&#10;        &quot;&quot;&quot;&#10;        Args:&#10;            tweets_list: 推文列表&#10;            labels: 标签列表&#10;            mode: 'train' | 'val' | 'test'&#10;                  所有阶段都过滤掉没有推文的样本&#10;        &quot;&quot;&quot;&#10;        self.mode = mode&#10;&#10;        # 所有阶段都过滤掉没有推文的样本&#10;        valid_indices = []&#10;        for idx, user_tweets in enumerate(tweets_list):&#10;            cleaned = self._clean_tweets(user_tweets)&#10;            if len(cleaned) &gt; 0:&#10;                valid_indices.append(idx)&#10;&#10;        self.tweets_list = [tweets_list[i] for i in valid_indices]&#10;        self.labels = [labels[i] for i in valid_indices]&#10;&#10;        filtered_count = len(tweets_list) - len(self.tweets_list)&#10;        print(f&quot;  [{mode}集] 有效样本: {len(self.tweets_list)}/{len(tweets_list)} (过滤 {filtered_count} 个无推文样本)&quot;)&#10;&#10;    def _clean_tweets(self, user_tweets):&#10;        &quot;&quot;&quot;清理推文文本&quot;&quot;&quot;&#10;        cleaned = []&#10;        if isinstance(user_tweets, list):&#10;            for tweet in user_tweets:&#10;                tweet_str = str(tweet).strip()&#10;                if tweet_str != '' and tweet_str != 'None':&#10;                    cleaned.append(tweet_str)&#10;        return cleaned&#10;&#10;    def __len__(self):&#10;        return len(self.tweets_list)&#10;&#10;    def __getitem__(self, idx):&#10;        user_tweets = self.tweets_list[idx]&#10;        label = self.labels[idx]&#10;&#10;        # 清理推文文本（保证非空）&#10;        cleaned_tweets = self._clean_tweets(user_tweets)&#10;&#10;        return {&#10;            'tweets_text': cleaned_tweets,&#10;            'label': torch.tensor(label, dtype=torch.float32)&#10;        }&#10;&#10;# 将一个 batch 中每个样本的推文文本列表和标签整理成字典供模型输入&#10;def collate_tweets_fn(batch):&#10;    tweets_text_lists = [item['tweets_text'] for item in batch]&#10;    labels = torch.stack([item['label'] for item in batch])&#10;&#10;    return {&#10;        'tweets_text_list': tweets_text_lists,&#10;        'label': labels&#10;    }&#10;&#10;def create_tweets_expert_config(&#10;    dataset_path=None,&#10;    batch_size=32,&#10;    learning_rate=1e-3,&#10;    device='cuda',&#10;    checkpoint_dir='../../autodl-fs/checkpoints',&#10;    roberta_model_name='distilroberta-base',&#10;    twibot_dataset=None&#10;):&#10;    &quot;&quot;&quot;&#10;    创建 Tweets Expert 配置&#10;&#10;    Args:&#10;        twibot_dataset: 预加载的Twibot20数据集对象（可选，避免重复加载）&#10;&#10;    Returns:&#10;        dict: 包含模型、数据加载器、优化器等的配置字典&#10;    &quot;&quot;&quot;&#10;    if dataset_path is None:&#10;        dataset_path = str(PROJECT_ROOT / 'processed_data')&#10;&#10;    print(f&quot;\n{'='*60}&quot;)&#10;    print(f&quot;配置 Tweets Expert&quot;)&#10;    print(f&quot;{'='*60}&quot;)&#10;&#10;    # 加载数据（如果没有预加载）&#10;    if twibot_dataset is None:&#10;        print(&quot;加载数据...&quot;)&#10;        twibot_dataset = Twibot20(root=dataset_path, device=device, process=True, save=True)&#10;    else:&#10;        print(&quot;使用预加载的数据集...&quot;)&#10;&#10;    tweets_list = twibot_dataset.tweets_preprogress()&#10;    labels = twibot_dataset.load_labels()&#10;&#10;    if isinstance(tweets_list, np.ndarray):&#10;        tweets_list = tweets_list.tolist()&#10;    labels = labels.cpu().numpy()&#10;&#10;    # 获取训练/验证/测试集索引&#10;    train_idx, val_idx, test_idx = twibot_dataset.train_val_test_mask()&#10;    train_idx = list(train_idx)&#10;    val_idx = list(val_idx)&#10;    test_idx = list(test_idx)&#10;&#10;    # 划分数据集&#10;    train_tweets = [tweets_list[i] for i in train_idx]&#10;    train_labels = labels[train_idx]&#10;&#10;    val_tweets = [tweets_list[i] for i in val_idx]&#10;    val_labels = labels[val_idx]&#10;&#10;    test_tweets = [tweets_list[i] for i in test_idx]&#10;    test_labels = labels[test_idx]&#10;&#10;    print(f&quot;  训练集: {len(train_tweets)} 样本&quot;)&#10;    print(f&quot;  验证集: {len(val_tweets)} 样本&quot;)&#10;    print(f&quot;  测试集: {len(test_tweets)} 样本&quot;)&#10;&#10;    # 统计推文数量&#10;    train_tweet_counts = [len(tweets) if isinstance(tweets, list) else 0 for tweets in train_tweets]&#10;    print(f&quot;  训练集平均推文数: {np.mean(train_tweet_counts):.2f}&quot;)&#10;&#10;    # 创建数据集和数据加载器&#10;    print(&quot;创建数据加载器...&quot;)&#10;    train_dataset = TweetsDataset(train_tweets, train_labels, mode='train')&#10;    val_dataset = TweetsDataset(val_tweets, val_labels, mode='val')&#10;    test_dataset = TweetsDataset(test_tweets, test_labels, mode='test')&#10;&#10;    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_tweets_fn)&#10;    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_tweets_fn)&#10;    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_tweets_fn)&#10;&#10;    # 初始化模型&#10;    print(f&quot;初始化模型 ({roberta_model_name})...&quot;)&#10;    model = TweetsExpert(roberta_model_name=roberta_model_name, device=device).to(device)&#10;    print(f&quot;  模型参数数量: {sum(p.numel() for p in model.parameters()):,}&quot;)&#10;    print(f&quot;  可训练参数数量: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}&quot;)&#10;&#10;    # 优化器和损失函数&#10;    optimizer = AdamW(model.parameters(), lr=learning_rate)&#10;    criterion = nn.BCELoss()&#10;&#10;    # 数据提取函数&#10;    def extract_fn(batch, device):&#10;        tweets_text_list = batch['tweets_text_list']&#10;        labels = batch['label'].to(device).unsqueeze(1)&#10;        return (tweets_text_list,), labels&#10;&#10;    return {&#10;        'name': 'tweets',&#10;        'model': model,&#10;        'train_loader': train_loader,&#10;        'val_loader': val_loader,&#10;        'test_loader': test_loader,&#10;        'optimizer': optimizer,&#10;        'criterion': criterion,&#10;        'device': device,&#10;        'checkpoint_dir': checkpoint_dir,&#10;        'extract_fn': extract_fn&#10;    }&#10;&#10;&#10;# ==================== Graph Expert ====================&#10;&#10;class GraphDataset(Dataset):&#10;    &quot;&quot;&quot;Graph Expert 数据集&quot;&quot;&quot;&#10;    def __init__(self, node_indices, labels):&#10;        &quot;&quot;&quot;&#10;        Args:&#10;            node_indices: 节点索引列表&#10;            labels: 标签列表&#10;        &quot;&quot;&quot;&#10;        self.node_indices = node_indices&#10;        self.labels = labels&#10;&#10;    def __len__(self):&#10;        return len(self.node_indices)&#10;&#10;    def __getitem__(self, idx):&#10;        return {&#10;            'node_index': torch.tensor(self.node_indices[idx], dtype=torch.long),&#10;            'label': torch.tensor(self.labels[idx], dtype=torch.float32)&#10;        }&#10;&#10;&#10;def collate_graph_fn(batch):&#10;    &quot;&quot;&quot;图数据 collate 函数&quot;&quot;&quot;&#10;    node_indices = torch.stack([item['node_index'] for item in batch])&#10;    labels = torch.stack([item['label'] for item in batch])&#10;&#10;    return {&#10;        'node_indices': node_indices,&#10;        'label': labels&#10;    }&#10;&#10;&#10;def create_graph_expert_config(&#10;    dataset_path=None,&#10;    batch_size=128,&#10;    learning_rate=1e-3,&#10;    weight_decay=1e-4,&#10;    device='cuda',&#10;    checkpoint_dir='../../autodl-fs/checkpoints',&#10;    hidden_dim=128,&#10;    expert_dim=64,&#10;    num_layers=2,&#10;    dropout=0.3,&#10;    expert_names=['des', 'tweets'],  # 用于聚合节点特征的专家列表&#10;    twibot_dataset=None&#10;):&#10;    &quot;&quot;&quot;&#10;    创建 Graph Expert 配置&#10;&#10;    节点特征从其他专家的表示聚合而来，使用 RGCN 进行图卷积&#10;&#10;    Args:&#10;        expert_names: 用于聚合节点特征的专家列表（必须已训练好）&#10;        twibot_dataset: 预加载的Twibot20数据集对象（可选，避免重复加载）&#10;&#10;    Returns:&#10;        dict: 包含模型、数据加载器、优化器等的配置字典&#10;    &quot;&quot;&quot;&#10;    if dataset_path is None:&#10;        dataset_path = str(PROJECT_ROOT / 'processed_data')&#10;&#10;    print(f&quot;\n{'='*60}&quot;)&#10;    print(f&quot;配置 Graph Expert (RGCN)&quot;)&#10;    print(f&quot;{'='*60}&quot;)&#10;    print(f&quot;  批次大小: {batch_size}&quot;)&#10;    print(f&quot;  学习率: {learning_rate}&quot;)&#10;    print(f&quot;  使用专家: {expert_names}&quot;)&#10;&#10;    # 加载数据（如果没有预加载）&#10;    if twibot_dataset is None:&#10;        print(&quot;\n加载数据集和图结构...&quot;)&#10;        twibot_dataset = Twibot20(root=dataset_path, device=device, process=True, save=True)&#10;    else:&#10;        print(&quot;\n使用预加载的数据集和图结构...&quot;)&#10;&#10;    labels = twibot_dataset.load_labels()&#10;    labels = labels.cpu().numpy()&#10;&#10;    # 构建图结构&#10;    edge_index, edge_type = twibot_dataset.build_graph()&#10;    print(f&quot;  图边数: {edge_index.shape[1]}&quot;)&#10;    print(f&quot;  边类型数: {len(torch.unique(edge_type))}&quot;)&#10;&#10;    # 获取训练/验证/测试集索引&#10;    train_idx, val_idx, test_idx = twibot_dataset.train_val_test_mask()&#10;    train_idx = list(train_idx)&#10;    val_idx = list(val_idx)&#10;    test_idx = list(test_idx)&#10;&#10;    print(f&quot;  训练集: {len(train_idx)} 节点&quot;)&#10;    print(f&quot;  验证集: {len(val_idx)} 节点&quot;)&#10;    print(f&quot;  测试集: {len(test_idx)} 节点&quot;)&#10;&#10;    # ========== 聚合节点特征：从其他专家提取表示 ==========&#10;    print(f&quot;\n{'='*60}&quot;)&#10;    print(f&quot;从其他专家聚合节点特征&quot;)&#10;    print(f&quot;{'='*60}&quot;)&#10;&#10;    from tqdm import tqdm&#10;&#10;    # 注意：需要对全部节点（包括支持集）提取特征&#10;    # df_data 包含所有节点（train + val + test + support）&#10;    num_all_nodes = len(twibot_dataset.df_data)&#10;    print(f&quot;  总节点数（含支持集）: {num_all_nodes}&quot;)&#10;&#10;    all_node_indices = list(range(num_all_nodes)) # 所有节点索引&#10;&#10;    # 定义缓存文件路径&#10;    cache_dir = Path(dataset_path) / 'graph_expert_cache'&#10;    cache_dir.mkdir(exist_ok=True)&#10;&#10;    # 生成缓存文件名（包含专家列表信息）&#10;    expert_names_str = '_'.join(sorted(expert_names))&#10;    aggregated_features_cache = cache_dir / f'aggregated_node_features_{expert_names_str}.pt'&#10;&#10;    # 检查是否存在缓存文件&#10;    if aggregated_features_cache.exists():&#10;        print(f&quot;\n  ✓ 发现缓存的聚合特征，直接加载...&quot;)&#10;        print(f&quot;    缓存路径: {aggregated_features_cache}&quot;)&#10;        initial_node_features = torch.load(aggregated_features_cache, map_location='cpu')&#10;        print(f&quot;  ✓ 加载完成，节点特征形状: {initial_node_features.shape}&quot;)&#10;    else:&#10;        print(f&quot;\n  未找到缓存，开始提取专家特征...&quot;)&#10;&#10;        # 存储所有节点的专家表示&#10;        all_expert_embeddings = []  # List of [num_all_nodes, 64]&#10;&#10;        for expert_name in expert_names:&#10;            # 检查单个专家的缓存&#10;            expert_cache_file = cache_dir / f'{expert_name}_node_features.pt'&#10;&#10;            if expert_cache_file.exists():&#10;                print(f&quot;\n  ✓ 加载 {expert_name} 专家的缓存特征...&quot;)&#10;                print(f&quot;    缓存路径: {expert_cache_file}&quot;)&#10;                cached_data = torch.load(expert_cache_file, map_location='cpu')&#10;                embeddings = cached_data['embeddings']&#10;                mask = cached_data['mask']&#10;                print(f&quot;    有效样本: {mask.sum().item()}/{len(mask)}&quot;)&#10;            else:&#10;                print(f&quot;\n  提取 {expert_name} 专家特征...&quot;)&#10;&#10;                # 提取该专家对所有节点的表示（包括支持集）&#10;                # 使用本地函数提取特征&#10;                embeddings, mask = _extract_expert_features_with_mask(&#10;                    expert_name, all_node_indices, dataset_path, checkpoint_dir, device,&#10;                    twibot_dataset=twibot_dataset  # 传入已加载的数据集，避免重复加载&#10;                )&#10;&#10;                # 保存单个专家的特征到缓存&#10;                print(f&quot;    保存 {expert_name} 特征到缓存...&quot;)&#10;                torch.save({&#10;                    'embeddings': embeddings.cpu(),&#10;                    'mask': mask.cpu()&#10;                }, expert_cache_file)&#10;                print(f&quot;    ✓ 已保存到: {expert_cache_file}&quot;)&#10;&#10;            all_expert_embeddings.append(embeddings)  # [num_all_nodes, 64]&#10;&#10;        # 聚合所有专家的表示：简单拼接后通过线性层&#10;        print(f&quot;\n  聚合 {len(expert_names)} 个专家的特征...&quot;)&#10;        stacked_embeddings = torch.stack(all_expert_embeddings, dim=1)  # [num_all_nodes, num_experts, 64]&#10;&#10;        # 方式1：平均池化&#10;        initial_node_features = torch.mean(stacked_embeddings, dim=1)  # [num_all_nodes, 64]&#10;&#10;        # 方式2：拼接（如果想用这个，需要修改 input_dim）&#10;        # initial_node_features = stacked_embeddings.view(num_all_nodes, -1)  # [num_all_nodes, num_experts*64]&#10;&#10;        print(f&quot;  初始节点特征形状: {initial_node_features.shape}&quot;)&#10;&#10;        # 保存聚合后的特征&#10;        print(f&quot;\n  保存聚合特征到缓存...&quot;)&#10;        torch.save(initial_node_features.cpu(), aggregated_features_cache)&#10;        print(f&quot;  ✓ 已保存到: {aggregated_features_cache}&quot;)&#10;&#10;    # 创建数据集和数据加载器（只用带标签的节点）&#10;    print(&quot;\n创建数据加载器...&quot;)&#10;    train_labels = labels[train_idx]&#10;    val_labels = labels[val_idx]&#10;    test_labels = labels[test_idx]&#10;&#10;    train_dataset = GraphDataset(train_idx, train_labels)&#10;    val_dataset = GraphDataset(val_idx, val_labels)&#10;    test_dataset = GraphDataset(test_idx, test_labels)&#10;&#10;    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_graph_fn)&#10;    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_graph_fn)&#10;    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_graph_fn)&#10;&#10;    # 初始化模型&#10;    print(&quot;\n初始化 Graph Expert 模型...&quot;)&#10;    model = GraphExpert(&#10;        num_nodes=num_all_nodes,&#10;        initial_node_features=initial_node_features,&#10;        num_relations=2,  # following (0) 和 follower (1)&#10;        hidden_dim=hidden_dim,&#10;        expert_dim=expert_dim,&#10;        num_layers=num_layers,&#10;        dropout=dropout,&#10;        device=device&#10;    ).to(device)&#10;&#10;    print(f&quot;  模型参数数量: {sum(p.numel() for p in model.parameters()):,}&quot;)&#10;    print(f&quot;  可训练参数数量: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}&quot;)&#10;&#10;    # 优化器和损失函数&#10;    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)&#10;    criterion = nn.BCELoss()&#10;&#10;    # 数据提取函数&#10;    def extract_fn(batch, device):&#10;        node_indices = batch['node_indices'].to(device)&#10;        labels = batch['label'].to(device).unsqueeze(1)&#10;        # 图结构是全局的，不随 batch 变化&#10;        return (node_indices, edge_index, edge_type), labels&#10;&#10;    return {&#10;        'name': 'graph',&#10;        'model': model,&#10;        'train_loader': train_loader,&#10;        'val_loader': val_loader,&#10;        'test_loader': test_loader,&#10;        'optimizer': optimizer,&#10;        'criterion': criterion,&#10;        'device': device,&#10;        'checkpoint_dir': checkpoint_dir,&#10;        'extract_fn': extract_fn,&#10;        'edge_index': edge_index,  # 保存图结构供后续使用&#10;        'edge_type': edge_type&#10;    }&#10;&#10;&#10;# ==================== 配置注册表 ====================&#10;&#10;EXPERT_CONFIGS = {&#10;    'des': create_des_expert_config,&#10;    'tweets': create_tweets_expert_config,&#10;    'graph': create_graph_expert_config,&#10;}&#10;&#10;# 获取专家配置&#10;def get_expert_config(expert_name, **kwargs):&#10;    &quot;&quot;&quot;&#10;    Args:&#10;        expert_name: 专家名称 ('des', 'tweets', 'graph', etc.)&#10;        **kwargs: 传递给配置函数的参数&#10;&#10;    Returns:&#10;        dict: 专家配置字典&#10;    &quot;&quot;&quot;&#10;    if expert_name not in EXPERT_CONFIGS:&#10;        raise ValueError(f&quot;未知的专家名称: {expert_name}. 可用选项: {list(EXPERT_CONFIGS.keys())}&quot;)&#10;&#10;    config_fn = EXPERT_CONFIGS[expert_name]&#10;&#10;    # 过滤kwargs，只传递专家配置函数接受的参数&#10;    import inspect&#10;    sig = inspect.signature(config_fn)&#10;    valid_params = set(sig.parameters.keys())&#10;    filtered_kwargs = {k: v for k, v in kwargs.items() if k in valid_params}&#10;&#10;    return config_fn(**filtered_kwargs)&#10;&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;专家配置模块&#10;定义各个专家的配置函数，包括数据集、模型、优化器等&#10;&quot;&quot;&quot;&#10;import torch&#10;import torch.nn as nn&#10;from torch.utils.data import Dataset, DataLoader&#10;from torch.optim import AdamW&#10;import numpy as np&#10;import sys&#10;from pathlib import Path&#10;&#10;# 添加项目根目录到路径&#10;sys.path.insert(0, str(Path(__file__).parent.parent))&#10;&#10;from src.dataset import Twibot20&#10;from src.model import DesExpert, DesExpertMoE, TweetsExpert, GraphExpert&#10;&#10;# 获取项目根目录&#10;PROJECT_ROOT = Path(__file__).parent.parent&#10;&#10;&#10;# ==================== 辅助函数 ====================&#10;&#10;def _extract_expert_features_with_mask(expert_name, node_indices, dataset_path, checkpoint_dir, device, twibot_dataset=None):&#10;    &quot;&quot;&quot;&#10;    从已训练的专家模型中提取特征&#10;&#10;    Args:&#10;        expert_name: 专家名称 ('des', 'tweets')&#10;        node_indices: 需要提取特征的节点索引列表&#10;        dataset_path: 数据集路径&#10;        checkpoint_dir: 模型检查点目录&#10;        device: 设备&#10;        twibot_dataset: 预加载的数据集（可选）&#10;&#10;    Returns:&#10;        embeddings: [num_nodes, expert_dim] 特征向量&#10;        mask: [num_nodes] bool tensor，标记哪些节点有有效特征&#10;    &quot;&quot;&quot;&#10;    from tqdm import tqdm&#10;&#10;    # 加载数据集（如果未提供）&#10;    if twibot_dataset is None:&#10;        twibot_dataset = Twibot20(root=dataset_path, device=device, process=True, save=True)&#10;&#10;    # 加载已训练的专家模型&#10;    checkpoint_path = Path(checkpoint_dir) / f'{expert_name}_expert_best.pth'&#10;    if not checkpoint_path.exists():&#10;        raise FileNotFoundError(f&quot;未找到专家模型检查点: {checkpoint_path}&quot;)&#10;&#10;    print(f&quot;    加载模型检查点: {checkpoint_path}&quot;)&#10;    checkpoint = torch.load(checkpoint_path, map_location=device)&#10;&#10;    # 根据专家类型创建模型并加载权重&#10;    if expert_name == 'des':&#10;        model = DesExpert(device=device).to(device)&#10;        model.load_state_dict(checkpoint['model_state_dict'])&#10;&#10;        # 获取描述数据&#10;        descriptions = twibot_dataset.Des_preprocess()&#10;        if isinstance(descriptions, np.ndarray):&#10;            descriptions = descriptions.tolist()&#10;&#10;        # 提取特征&#10;        model.eval()&#10;        embeddings_list = []&#10;        valid_mask = []&#10;&#10;        batch_size = 32&#10;        with torch.no_grad():&#10;            for i in tqdm(range(0, len(node_indices), batch_size), desc=f&quot;  提取 {expert_name} 特征&quot;):&#10;                batch_indices = node_indices[i:i+batch_size]&#10;                batch_descriptions = [descriptions[idx] for idx in batch_indices]&#10;&#10;                # 检查是否有有效描述&#10;                batch_valid = []&#10;                for desc in batch_descriptions:&#10;                    desc_str = str(desc).strip()&#10;                    is_valid = desc_str != '' and desc_str.lower() != 'none'&#10;                    batch_valid.append(is_valid)&#10;&#10;                # 提取特征&#10;                expert_repr, _ = model(batch_descriptions)&#10;                embeddings_list.append(expert_repr.cpu())&#10;                valid_mask.extend(batch_valid)&#10;&#10;        embeddings = torch.cat(embeddings_list, dim=0)&#10;        mask = torch.tensor(valid_mask, dtype=torch.bool)&#10;&#10;    elif expert_name == 'tweets':&#10;        model = TweetsExpert(device=device).to(device)&#10;        model.load_state_dict(checkpoint['model_state_dict'])&#10;&#10;        # 获取推文数据&#10;        tweets_list = twibot_dataset.tweets_preprogress()&#10;        if isinstance(tweets_list, np.ndarray):&#10;            tweets_list = tweets_list.tolist()&#10;&#10;        # 提取特征&#10;        model.eval()&#10;        embeddings_list = []&#10;        valid_mask = []&#10;&#10;        batch_size = 32&#10;        with torch.no_grad():&#10;            for i in tqdm(range(0, len(node_indices), batch_size), desc=f&quot;  提取 {expert_name} 特征&quot;):&#10;                batch_indices = node_indices[i:i+batch_size]&#10;                batch_tweets = [tweets_list[idx] for idx in batch_indices]&#10;&#10;                # 检查是否有有效推文&#10;                batch_valid = []&#10;                for user_tweets in batch_tweets:&#10;                    if isinstance(user_tweets, list) and len(user_tweets) &gt; 0:&#10;                        cleaned = [str(t).strip() for t in user_tweets if str(t).strip() != '' and str(t).strip() != 'None']&#10;                        is_valid = len(cleaned) &gt; 0&#10;                    else:&#10;                        is_valid = False&#10;                    batch_valid.append(is_valid)&#10;&#10;                # 提取特征&#10;                expert_repr, _ = model(batch_tweets)&#10;                embeddings_list.append(expert_repr.cpu())&#10;                valid_mask.extend(batch_valid)&#10;&#10;        embeddings = torch.cat(embeddings_list, dim=0)&#10;        mask = torch.tensor(valid_mask, dtype=torch.bool)&#10;&#10;    else:&#10;        raise ValueError(f&quot;不支持的专家类型: {expert_name}&quot;)&#10;&#10;    print(f&quot;    特征形状: {embeddings.shape}, 有效样本: {mask.sum().item()}/{len(mask)}&quot;)&#10;&#10;    return embeddings, mask&#10;&#10;&#10;# ==================== Description Expert ====================&#10;&#10;class DescriptionDataset(Dataset):&#10;    &quot;&quot;&quot;Description 专家数据集&quot;&quot;&quot;&#10;    def __init__(self, descriptions, labels, mode='train'):&#10;        &quot;&quot;&quot;&#10;        Args:&#10;            descriptions: 简介列表&#10;            labels: 标签列表&#10;            mode: 'train' | 'val' | 'test'&#10;                  所有阶段都过滤掉空简介的样本&#10;        &quot;&quot;&quot;&#10;        self.mode = mode&#10;&#10;        # 所有阶段都过滤掉空简介的样本&#10;        valid_indices = []&#10;        for idx, desc in enumerate(descriptions):&#10;            desc_str = str(desc).strip()&#10;            if desc_str != '' and desc_str.lower() != 'none':&#10;                valid_indices.append(idx)&#10;&#10;        self.descriptions = [descriptions[i] for i in valid_indices]&#10;        self.labels = [labels[i] for i in valid_indices]&#10;&#10;        filtered_count = len(descriptions) - len(self.descriptions)&#10;        print(f&quot;  [{mode}集] 有效样本: {len(self.descriptions)}/{len(descriptions)} (过滤 {filtered_count} 个空简介样本)&quot;)&#10;&#10;    def __len__(self):&#10;        return len(self.descriptions)&#10;&#10;    def __getitem__(self, idx):&#10;        description = str(self.descriptions[idx])&#10;        label = self.labels[idx]&#10;&#10;        return {&#10;            'description_text': description,&#10;            'label': torch.tensor(label, dtype=torch.float32)&#10;        }&#10;&#10;&#10;def create_des_expert_config(&#10;    dataset_path=None,&#10;    batch_size=32,&#10;    learning_rate=5e-4,&#10;    weight_decay=0.01,&#10;    device='cuda',&#10;    checkpoint_dir='../../autodl-fs/model',&#10;    model_name='microsoft/deberta-v3-base',&#10;    max_grad_norm=1.0,&#10;    dropout=0.3,&#10;    early_stopping_patience=4,&#10;    num_experts=4,  # MoE 中的专家数量（默认4个）&#10;    top_k=2,        # Top-K 选择，每次只用权重最大的K个专家（默认2个）&#10;    twibot_dataset=None&#10;):&#10;    &quot;&quot;&quot;&#10;    创建 Description Expert 配置 (使用 DeBERTa-v3-base + MoE + Top-K)&#10;&#10;    Args:&#10;        model_name: 预训练模型名称，默认为 'microsoft/deberta-v3-base'&#10;        batch_size: 批次大小，默认32&#10;        learning_rate: 学习率，默认5e-4 (0.0005)&#10;        weight_decay: 权重衰减，默认0.01&#10;        dropout: Dropout率，默认0.3&#10;        max_grad_norm: 梯度裁剪阈值，默认1.0&#10;        early_stopping_patience: 早停耐心值，默认4&#10;        num_experts: MoE 中的专家数量，默认4&#10;        top_k: 每次选择的专家数量（Top-K），默认2&#10;        twibot_dataset: 预加载的Twibot20数据集对象（可选，避免重复加载）&#10;&#10;    Returns:&#10;        dict: 包含模型、数据加载器、优化器等的配置字典&#10;    &quot;&quot;&quot;&#10;    if dataset_path is None:&#10;        dataset_path = str(PROJECT_ROOT / 'processed_data')&#10;&#10;    print(f&quot;\n{'='*60}&quot;)&#10;    print(f&quot;配置 Description Expert with MoE + Top-K (DeBERTa-v3-base)&quot;)&#10;    print(f&quot;{'='*60}&quot;)&#10;    print(f&quot;  模型: {model_name}&quot;)&#10;    print(f&quot;  专家数量: {num_experts}&quot;)&#10;    print(f&quot;  Top-K 选择: {top_k} (每次只用权重最大的{top_k}个专家)&quot;)&#10;    print(f&quot;  批次大小: {batch_size}&quot;)&#10;    print(f&quot;  学习率: {learning_rate}&quot;)&#10;    print(f&quot;  权重衰减: {weight_decay}&quot;)&#10;    print(f&quot;  Dropout: {dropout}&quot;)&#10;    print(f&quot;  梯度裁剪: {max_grad_norm}&quot;)&#10;    print(f&quot;  早停耐心值: {early_stopping_patience}&quot;)&#10;&#10;    # 加载数据（如果没有预加载）&#10;    if twibot_dataset is None:&#10;        print(&quot;加载数据...&quot;)&#10;        twibot_dataset = Twibot20(root=dataset_path, device=device, process=True, save=True)&#10;    else:&#10;        print(&quot;使用预加载的数据集...&quot;)&#10;&#10;    descriptions = twibot_dataset.Des_preprocess()&#10;    labels = twibot_dataset.load_labels()&#10;&#10;    if isinstance(descriptions, np.ndarray):&#10;        descriptions = descriptions.tolist()&#10;    labels = labels.cpu().numpy()&#10;&#10;    # 获取训练/验证/测试集索引&#10;    train_idx, val_idx, test_idx = twibot_dataset.train_val_test_mask()&#10;    train_idx = list(train_idx)&#10;    val_idx = list(val_idx)&#10;    test_idx = list(test_idx)&#10;&#10;    # 划分数据集&#10;    train_descriptions = [descriptions[i] for i in train_idx]&#10;    train_labels = labels[train_idx]&#10;&#10;    val_descriptions = [descriptions[i] for i in val_idx]&#10;    val_labels = labels[val_idx]&#10;&#10;    test_descriptions = [descriptions[i] for i in test_idx]&#10;    test_labels = labels[test_idx]&#10;&#10;    print(f&quot;  训练集: {len(train_descriptions)} 样本&quot;)&#10;    print(f&quot;  验证集: {len(val_descriptions)} 样本&quot;)&#10;    print(f&quot;  测试集: {len(test_descriptions)} 样本&quot;)&#10;&#10;    # 创建数据集和数据加载器&#10;    print(&quot;创建数据加载器...&quot;)&#10;    train_dataset = DescriptionDataset(train_descriptions, train_labels, mode='train')&#10;    val_dataset = DescriptionDataset(val_descriptions, val_labels, mode='val')&#10;    test_dataset = DescriptionDataset(test_descriptions, test_labels, mode='test')&#10;&#10;    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)&#10;    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)&#10;    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)&#10;&#10;    # 初始化模型 (使用 MoE + Top-K 版本)&#10;    print(&quot;初始化 MoE + Top-K 模型...&quot;)&#10;    model = DesExpertMoE(&#10;        model_name=model_name,&#10;        device=device,&#10;        dropout=dropout,&#10;        num_experts=num_experts,&#10;        top_k=top_k,&#10;        expert_dim=64,&#10;        hidden_dim=768&#10;    ).to(device)&#10;    print(f&quot;  模型参数数量: {sum(p.numel() for p in model.parameters()):,}&quot;)&#10;    print(f&quot;  可训练参数数量: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}&quot;)&#10;    print(f&quot;  - Gating Network + {num_experts} 个 MLP 专家 (Top-{top_k} 选择)&quot;)&#10;&#10;    # 优化器和损失函数 - 使用权重衰减&#10;    optimizer = AdamW(&#10;        filter(lambda p: p.requires_grad, model.parameters()),&#10;        lr=learning_rate,&#10;        weight_decay=weight_decay,&#10;        betas=(0.9, 0.999),&#10;        eps=1e-8&#10;    )&#10;    criterion = nn.BCELoss()&#10;&#10;    # 数据提取函数&#10;    def extract_fn(batch, device):&#10;        description_texts = batch['description_text']&#10;        labels = batch['label'].to(device).unsqueeze(1)&#10;        return (description_texts,), labels&#10;&#10;    return {&#10;        'name': 'des',&#10;        'model': model,&#10;        'train_loader': train_loader,&#10;        'val_loader': val_loader,&#10;        'test_loader': test_loader,&#10;        'optimizer': optimizer,&#10;        'criterion': criterion,&#10;        'device': device,&#10;        'checkpoint_dir': checkpoint_dir,&#10;        'extract_fn': extract_fn,&#10;        'max_grad_norm': max_grad_norm,  # 梯度裁剪&#10;        'early_stopping_patience': early_stopping_patience,  # 早停&#10;        'batch_size': batch_size,&#10;        'learning_rate': learning_rate,&#10;        'dropout': dropout,&#10;        'num_experts': num_experts,  # MoE 专家数量&#10;        'top_k': top_k  # Top-K 选择&#10;    }&#10;&#10;&#10;# ==================== Tweets Expert ====================&#10;&#10;class TweetsDataset(Dataset):&#10;    def __init__(self, tweets_list, labels, mode='train'):&#10;        &quot;&quot;&quot;&#10;        Args:&#10;            tweets_list: 推文列表&#10;            labels: 标签列表&#10;            mode: 'train' | 'val' | 'test'&#10;                  所有阶段都过滤掉没有推文的样本&#10;        &quot;&quot;&quot;&#10;        self.mode = mode&#10;&#10;        # 所有阶段都过滤掉没有推文的样本&#10;        valid_indices = []&#10;        for idx, user_tweets in enumerate(tweets_list):&#10;            cleaned = self._clean_tweets(user_tweets)&#10;            if len(cleaned) &gt; 0:&#10;                valid_indices.append(idx)&#10;&#10;        self.tweets_list = [tweets_list[i] for i in valid_indices]&#10;        self.labels = [labels[i] for i in valid_indices]&#10;&#10;        filtered_count = len(tweets_list) - len(self.tweets_list)&#10;        print(f&quot;  [{mode}集] 有效样本: {len(self.tweets_list)}/{len(tweets_list)} (过滤 {filtered_count} 个无推文样本)&quot;)&#10;&#10;    def _clean_tweets(self, user_tweets):&#10;        &quot;&quot;&quot;清理推文文本&quot;&quot;&quot;&#10;        cleaned = []&#10;        if isinstance(user_tweets, list):&#10;            for tweet in user_tweets:&#10;                tweet_str = str(tweet).strip()&#10;                if tweet_str != '' and tweet_str != 'None':&#10;                    cleaned.append(tweet_str)&#10;        return cleaned&#10;&#10;    def __len__(self):&#10;        return len(self.tweets_list)&#10;&#10;    def __getitem__(self, idx):&#10;        user_tweets = self.tweets_list[idx]&#10;        label = self.labels[idx]&#10;&#10;        # 清理推文文本（保证非空）&#10;        cleaned_tweets = self._clean_tweets(user_tweets)&#10;&#10;        return {&#10;            'tweets_text': cleaned_tweets,&#10;            'label': torch.tensor(label, dtype=torch.float32)&#10;        }&#10;&#10;# 将一个 batch 中每个样本的推文文本列表和标签整理成字典供模型输入&#10;def collate_tweets_fn(batch):&#10;    tweets_text_lists = [item['tweets_text'] for item in batch]&#10;    labels = torch.stack([item['label'] for item in batch])&#10;&#10;    return {&#10;        'tweets_text_list': tweets_text_lists,&#10;        'label': labels&#10;    }&#10;&#10;def create_tweets_expert_config(&#10;    dataset_path=None,&#10;    batch_size=32,&#10;    learning_rate=1e-3,&#10;    device='cuda',&#10;    checkpoint_dir='../../autodl-fs/model',&#10;    roberta_model_name='distilroberta-base',&#10;    twibot_dataset=None&#10;):&#10;    &quot;&quot;&quot;&#10;    创建 Tweets Expert 配置&#10;&#10;    Args:&#10;        twibot_dataset: 预加载的Twibot20数据集对象（可选，避免重复加载）&#10;&#10;    Returns:&#10;        dict: 包含模型、数据加载器、优化器等的配置字典&#10;    &quot;&quot;&quot;&#10;    if dataset_path is None:&#10;        dataset_path = str(PROJECT_ROOT / 'processed_data')&#10;&#10;    print(f&quot;\n{'='*60}&quot;)&#10;    print(f&quot;配置 Tweets Expert&quot;)&#10;    print(f&quot;{'='*60}&quot;)&#10;&#10;    # 加载数据（如果没有预加载）&#10;    if twibot_dataset is None:&#10;        print(&quot;加载数据...&quot;)&#10;        twibot_dataset = Twibot20(root=dataset_path, device=device, process=True, save=True)&#10;    else:&#10;        print(&quot;使用预加载的数据集...&quot;)&#10;&#10;    tweets_list = twibot_dataset.tweets_preprogress()&#10;    labels = twibot_dataset.load_labels()&#10;&#10;    if isinstance(tweets_list, np.ndarray):&#10;        tweets_list = tweets_list.tolist()&#10;    labels = labels.cpu().numpy()&#10;&#10;    # 获取训练/验证/测试集索引&#10;    train_idx, val_idx, test_idx = twibot_dataset.train_val_test_mask()&#10;    train_idx = list(train_idx)&#10;    val_idx = list(val_idx)&#10;    test_idx = list(test_idx)&#10;&#10;    # 划分数据集&#10;    train_tweets = [tweets_list[i] for i in train_idx]&#10;    train_labels = labels[train_idx]&#10;&#10;    val_tweets = [tweets_list[i] for i in val_idx]&#10;    val_labels = labels[val_idx]&#10;&#10;    test_tweets = [tweets_list[i] for i in test_idx]&#10;    test_labels = labels[test_idx]&#10;&#10;    print(f&quot;  训练集: {len(train_tweets)} 样本&quot;)&#10;    print(f&quot;  验证集: {len(val_tweets)} 样本&quot;)&#10;    print(f&quot;  测试集: {len(test_tweets)} 样本&quot;)&#10;&#10;    # 统计推文数量&#10;    train_tweet_counts = [len(tweets) if isinstance(tweets, list) else 0 for tweets in train_tweets]&#10;    print(f&quot;  训练集平均推文数: {np.mean(train_tweet_counts):.2f}&quot;)&#10;&#10;    # 创建数据集和数据加载器&#10;    print(&quot;创建数据加载器...&quot;)&#10;    train_dataset = TweetsDataset(train_tweets, train_labels, mode='train')&#10;    val_dataset = TweetsDataset(val_tweets, val_labels, mode='val')&#10;    test_dataset = TweetsDataset(test_tweets, test_labels, mode='test')&#10;&#10;    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_tweets_fn)&#10;    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_tweets_fn)&#10;    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_tweets_fn)&#10;&#10;    # 初始化模型&#10;    print(f&quot;初始化模型 ({roberta_model_name})...&quot;)&#10;    model = TweetsExpert(roberta_model_name=roberta_model_name, device=device).to(device)&#10;    print(f&quot;  模型参数数量: {sum(p.numel() for p in model.parameters()):,}&quot;)&#10;    print(f&quot;  可训练参数数量: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}&quot;)&#10;&#10;    # 优化器和损失函数&#10;    optimizer = AdamW(model.parameters(), lr=learning_rate)&#10;    criterion = nn.BCELoss()&#10;&#10;    # 数据提取函数&#10;    def extract_fn(batch, device):&#10;        tweets_text_list = batch['tweets_text_list']&#10;        labels = batch['label'].to(device).unsqueeze(1)&#10;        return (tweets_text_list,), labels&#10;&#10;    return {&#10;        'name': 'tweets',&#10;        'model': model,&#10;        'train_loader': train_loader,&#10;        'val_loader': val_loader,&#10;        'test_loader': test_loader,&#10;        'optimizer': optimizer,&#10;        'criterion': criterion,&#10;        'device': device,&#10;        'checkpoint_dir': checkpoint_dir,&#10;        'extract_fn': extract_fn&#10;    }&#10;&#10;&#10;# ==================== Graph Expert ====================&#10;&#10;class GraphDataset(Dataset):&#10;    &quot;&quot;&quot;Graph Expert 数据集&quot;&quot;&quot;&#10;    def __init__(self, node_indices, labels):&#10;        &quot;&quot;&quot;&#10;        Args:&#10;            node_indices: 节点索引列表&#10;            labels: 标签列表&#10;        &quot;&quot;&quot;&#10;        self.node_indices = node_indices&#10;        self.labels = labels&#10;&#10;    def __len__(self):&#10;        return len(self.node_indices)&#10;&#10;    def __getitem__(self, idx):&#10;        return {&#10;            'node_index': torch.tensor(self.node_indices[idx], dtype=torch.long),&#10;            'label': torch.tensor(self.labels[idx], dtype=torch.float32)&#10;        }&#10;&#10;&#10;def collate_graph_fn(batch):&#10;    &quot;&quot;&quot;图数据 collate 函数&quot;&quot;&quot;&#10;    node_indices = torch.stack([item['node_index'] for item in batch])&#10;    labels = torch.stack([item['label'] for item in batch])&#10;&#10;    return {&#10;        'node_indices': node_indices,&#10;        'label': labels&#10;    }&#10;&#10;&#10;def create_graph_expert_config(&#10;    dataset_path=None,&#10;    batch_size=128,&#10;    learning_rate=1e-3,&#10;    weight_decay=1e-4,&#10;    device='cuda',&#10;    checkpoint_dir='../../autodl-fs/model',&#10;    hidden_dim=128,&#10;    expert_dim=64,&#10;    num_layers=2,&#10;    dropout=0.3,&#10;    expert_names=['des', 'tweets'],  # 用于聚合节点特征的专家列表&#10;    twibot_dataset=None&#10;):&#10;    &quot;&quot;&quot;&#10;    创建 Graph Expert 配置&#10;&#10;    节点特征从其他专家的表示聚合而来，使用 RGCN 进行图卷积&#10;&#10;    Args:&#10;        expert_names: 用于聚合节点特征的专家列表（必须已训练好）&#10;        twibot_dataset: 预加载的Twibot20数据集对象（可选，避免重复加载）&#10;&#10;    Returns:&#10;        dict: 包含模型、数据加载器、优化器等的配置字典&#10;    &quot;&quot;&quot;&#10;    if dataset_path is None:&#10;        dataset_path = str(PROJECT_ROOT / 'processed_data')&#10;&#10;    print(f&quot;\n{'='*60}&quot;)&#10;    print(f&quot;配置 Graph Expert (RGCN)&quot;)&#10;    print(f&quot;{'='*60}&quot;)&#10;    print(f&quot;  批次大小: {batch_size}&quot;)&#10;    print(f&quot;  学习率: {learning_rate}&quot;)&#10;    print(f&quot;  使用专家: {expert_names}&quot;)&#10;&#10;    # 加载数据（如果没有预加载）&#10;    if twibot_dataset is None:&#10;        print(&quot;\n加载数据集和图结构...&quot;)&#10;        twibot_dataset = Twibot20(root=dataset_path, device=device, process=True, save=True)&#10;    else:&#10;        print(&quot;\n使用预加载的数据集和图结构...&quot;)&#10;&#10;    labels = twibot_dataset.load_labels()&#10;    labels = labels.cpu().numpy()&#10;&#10;    # 构建图结构&#10;    edge_index, edge_type = twibot_dataset.build_graph()&#10;    print(f&quot;  图边数: {edge_index.shape[1]}&quot;)&#10;    print(f&quot;  边类型数: {len(torch.unique(edge_type))}&quot;)&#10;&#10;    # 获取训练/验证/测试集索引&#10;    train_idx, val_idx, test_idx = twibot_dataset.train_val_test_mask()&#10;    train_idx = list(train_idx)&#10;    val_idx = list(val_idx)&#10;    test_idx = list(test_idx)&#10;&#10;    print(f&quot;  训练集: {len(train_idx)} 节点&quot;)&#10;    print(f&quot;  验证集: {len(val_idx)} 节点&quot;)&#10;    print(f&quot;  测试集: {len(test_idx)} 节点&quot;)&#10;&#10;    # ========== 聚合节点特征：从其他专家提取表示 ==========&#10;    print(f&quot;\n{'='*60}&quot;)&#10;    print(f&quot;从其他专家聚合节点特征&quot;)&#10;    print(f&quot;{'='*60}&quot;)&#10;&#10;    from tqdm import tqdm&#10;&#10;    # 注意：需要对全部节点（包括支持集）提取特征&#10;    # df_data 包含所有节点（train + val + test + support）&#10;    num_all_nodes = len(twibot_dataset.df_data)&#10;    print(f&quot;  总节点数（含支持集）: {num_all_nodes}&quot;)&#10;&#10;    all_node_indices = list(range(num_all_nodes)) # 所有节点索引&#10;&#10;    # 定义缓存文件路径&#10;    cache_dir = Path(dataset_path) / 'graph_expert_cache'&#10;    cache_dir.mkdir(exist_ok=True)&#10;&#10;    # 生成缓存文件名（包含专家列表信息）&#10;    expert_names_str = '_'.join(sorted(expert_names))&#10;    aggregated_features_cache = cache_dir / f'aggregated_node_features_{expert_names_str}.pt'&#10;&#10;    # 检查是否存在缓存文件&#10;    if aggregated_features_cache.exists():&#10;        print(f&quot;\n  ✓ 发现缓存的聚合特征，直接加载...&quot;)&#10;        print(f&quot;    缓存路径: {aggregated_features_cache}&quot;)&#10;        initial_node_features = torch.load(aggregated_features_cache, map_location='cpu')&#10;        print(f&quot;  ✓ 加载完成，节点特征形状: {initial_node_features.shape}&quot;)&#10;    else:&#10;        print(f&quot;\n  未找到缓存，开始提取专家特征...&quot;)&#10;&#10;        # 存储所有节点的专家表示&#10;        all_expert_embeddings = []  # List of [num_all_nodes, 64]&#10;&#10;        for expert_name in expert_names:&#10;            # 检查单个专家的缓存&#10;            expert_cache_file = cache_dir / f'{expert_name}_node_features.pt'&#10;&#10;            if expert_cache_file.exists():&#10;                print(f&quot;\n  ✓ 加载 {expert_name} 专家的缓存特征...&quot;)&#10;                print(f&quot;    缓存路径: {expert_cache_file}&quot;)&#10;                cached_data = torch.load(expert_cache_file, map_location='cpu')&#10;                embeddings = cached_data['embeddings']&#10;                mask = cached_data['mask']&#10;                print(f&quot;    有效样本: {mask.sum().item()}/{len(mask)}&quot;)&#10;            else:&#10;                print(f&quot;\n  提取 {expert_name} 专家特征...&quot;)&#10;&#10;                # 提取该专家对所有节点的表示（包括支持集）&#10;                # 使用本地函数提取特征&#10;                embeddings, mask = _extract_expert_features_with_mask(&#10;                    expert_name, all_node_indices, dataset_path, checkpoint_dir, device,&#10;                    twibot_dataset=twibot_dataset  # 传入已加载的数据集，避免重复加载&#10;                )&#10;&#10;                # 保存单个专家的特征到缓存&#10;                print(f&quot;    保存 {expert_name} 特征到缓存...&quot;)&#10;                torch.save({&#10;                    'embeddings': embeddings.cpu(),&#10;                    'mask': mask.cpu()&#10;                }, expert_cache_file)&#10;                print(f&quot;    ✓ 已保存到: {expert_cache_file}&quot;)&#10;&#10;            all_expert_embeddings.append(embeddings)  # [num_all_nodes, 64]&#10;&#10;        # 聚合所有专家的表示：简单拼接后通过线性层&#10;        print(f&quot;\n  聚合 {len(expert_names)} 个专家的特征...&quot;)&#10;        stacked_embeddings = torch.stack(all_expert_embeddings, dim=1)  # [num_all_nodes, num_experts, 64]&#10;&#10;        # 方式1：平均池化&#10;        initial_node_features = torch.mean(stacked_embeddings, dim=1)  # [num_all_nodes, 64]&#10;&#10;        # 方式2：拼接（如果想用这个，需要修改 input_dim）&#10;        # initial_node_features = stacked_embeddings.view(num_all_nodes, -1)  # [num_all_nodes, num_experts*64]&#10;&#10;        print(f&quot;  初始节点特征形状: {initial_node_features.shape}&quot;)&#10;&#10;        # 保存聚合后的特征&#10;        print(f&quot;\n  保存聚合特征到缓存...&quot;)&#10;        torch.save(initial_node_features.cpu(), aggregated_features_cache)&#10;        print(f&quot;  ✓ 已保存到: {aggregated_features_cache}&quot;)&#10;&#10;    # 创建数据集和数据加载器（只用带标签的节点）&#10;    print(&quot;\n创建数据加载器...&quot;)&#10;    train_labels = labels[train_idx]&#10;    val_labels = labels[val_idx]&#10;    test_labels = labels[test_idx]&#10;&#10;    train_dataset = GraphDataset(train_idx, train_labels)&#10;    val_dataset = GraphDataset(val_idx, val_labels)&#10;    test_dataset = GraphDataset(test_idx, test_labels)&#10;&#10;    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_graph_fn)&#10;    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_graph_fn)&#10;    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_graph_fn)&#10;&#10;    # 初始化模型&#10;    print(&quot;\n初始化 Graph Expert 模型...&quot;)&#10;    model = GraphExpert(&#10;        num_nodes=num_all_nodes,&#10;        initial_node_features=initial_node_features,&#10;        num_relations=2,  # following (0) 和 follower (1)&#10;        hidden_dim=hidden_dim,&#10;        expert_dim=expert_dim,&#10;        num_layers=num_layers,&#10;        dropout=dropout,&#10;        device=device&#10;    ).to(device)&#10;&#10;    print(f&quot;  模型参数数量: {sum(p.numel() for p in model.parameters()):,}&quot;)&#10;    print(f&quot;  可训练参数数量: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}&quot;)&#10;&#10;    # 优化器和损失函数&#10;    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)&#10;    criterion = nn.BCELoss()&#10;&#10;    # 数据提取函数&#10;    def extract_fn(batch, device):&#10;        node_indices = batch['node_indices'].to(device)&#10;        labels = batch['label'].to(device).unsqueeze(1)&#10;        # 图结构是全局的，不随 batch 变化&#10;        return (node_indices, edge_index, edge_type), labels&#10;&#10;    return {&#10;        'name': 'graph',&#10;        'model': model,&#10;        'train_loader': train_loader,&#10;        'val_loader': val_loader,&#10;        'test_loader': test_loader,&#10;        'optimizer': optimizer,&#10;        'criterion': criterion,&#10;        'device': device,&#10;        'checkpoint_dir': checkpoint_dir,&#10;        'extract_fn': extract_fn,&#10;        'edge_index': edge_index,  # 保存图结构供后续使用&#10;        'edge_type': edge_type&#10;    }&#10;&#10;&#10;# ==================== 配置注册表 ====================&#10;&#10;EXPERT_CONFIGS = {&#10;    'des': create_des_expert_config,&#10;    'tweets': create_tweets_expert_config,&#10;    'graph': create_graph_expert_config,&#10;}&#10;&#10;# 获取专家配置&#10;def get_expert_config(expert_name, **kwargs):&#10;    &quot;&quot;&quot;&#10;    Args:&#10;        expert_name: 专家名称 ('des', 'tweets', 'graph', etc.)&#10;        **kwargs: 传递给配置函数的参数&#10;&#10;    Returns:&#10;        dict: 专家配置字典&#10;    &quot;&quot;&quot;&#10;    if expert_name not in EXPERT_CONFIGS:&#10;        raise ValueError(f&quot;未知的专家名称: {expert_name}. 可用选项: {list(EXPERT_CONFIGS.keys())}&quot;)&#10;&#10;    config_fn = EXPERT_CONFIGS[expert_name]&#10;&#10;    # 过滤kwargs，只传递专家配置函数接受的参数&#10;    import inspect&#10;    sig = inspect.signature(config_fn)&#10;    valid_params = set(sig.parameters.keys())&#10;    filtered_kwargs = {k: v for k, v in kwargs.items() if k in valid_params}&#10;&#10;    return config_fn(**filtered_kwargs)&#10;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/scripts/train_experts.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/scripts/train_experts.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;统一专家训练入口&#10;使用通用训练器训练所有专家模型&#10;&quot;&quot;&quot;&#10;import torch&#10;import argparse&#10;import sys&#10;from pathlib import Path&#10;import os&#10;os.environ[&quot;HF_ENDPOINT&quot;] = &quot;https://hf-mirror.com&quot;&#10;&#10;&#10;# 添加项目根目录到路径&#10;project_root = Path(__file__).parent.parent&#10;sys.path.insert(0, str(project_root))&#10;&#10;from expert_trainer import ExpertTrainer&#10;from configs.expert_configs import get_expert_config&#10;&#10;# 训练单个专家&#10;def train_single_expert(expert_name, config_params, num_epochs=10, save_embeddings=True, embeddings_dir='../../autodl-fs/labeled_embedding'):&#10;    # 获取专家配置&#10;    config = get_expert_config(expert_name, **config_params)&#10;&#10;    # 创建训练器&#10;    trainer = ExpertTrainer(config)&#10;&#10;    # 开始训练&#10;    history = trainer.train(num_epochs, save_embeddings=save_embeddings, embeddings_dir=embeddings_dir)&#10;&#10;    return history&#10;&#10;&#10;def check_expert_dependencies(expert_name, checkpoint_dir):&#10;    &quot;&quot;&quot;&#10;    检查专家的依赖是否满足&#10;&#10;    Args:&#10;        expert_name: 专家名称&#10;        checkpoint_dir: 检查点目录&#10;&#10;    Returns:&#10;        bool: 依赖是否满足&#10;    &quot;&quot;&quot;&#10;    # 定义专家依赖关系&#10;    dependencies = {&#10;        'graph': ['des', 'tweets'],  # 图专家依赖 des 和 tweets&#10;    }&#10;&#10;    if expert_name not in dependencies:&#10;        return True  # 没有依赖，直接返回True&#10;&#10;    required_experts = dependencies[expert_name]&#10;    checkpoint_dir = Path(checkpoint_dir)&#10;&#10;    print(f&quot;  检查 {expert_name} 专家的依赖...&quot;)&#10;    for dep_expert in required_experts:&#10;        checkpoint_path = checkpoint_dir / f'{dep_expert}_expert_best.pt'&#10;        if not checkpoint_path.exists():&#10;            print(f&quot;    ✗ 缺少依赖: {dep_expert} 专家未训练&quot;)&#10;            print(f&quot;    请先训练 {dep_expert} 专家&quot;)&#10;            return False&#10;        else:&#10;            print(f&quot;    ✓ {dep_expert} 专家已训练&quot;)&#10;&#10;    return True&#10;&#10;&#10;def train_all_experts(config_params, num_epochs=10, experts=None, save_embeddings=True, embeddings_dir='../../autodl-fs/labeled_embedding'):&#10;    &quot;&quot;&quot;&#10;    训练所有专家或指定的专家列表&#10;&#10;    Args:&#10;        config_params: 配置参数字典&#10;        num_epochs: 训练轮数&#10;        experts: 要训练的专家列表，None表示训练所有可用专家&#10;        save_embeddings: 是否保存特征嵌入&#10;        embeddings_dir: 特征嵌入保存目录&#10;&#10;    Returns:&#10;        dict: 所有专家的训练结果&#10;    &quot;&quot;&quot;&#10;    # 默认训练的专家列表（按依赖顺序）&#10;    if experts is None:&#10;        experts = ['des', 'tweets', 'graph']  # graph 依赖 des 和 tweets，所以放最后&#10;&#10;    results = {}&#10;&#10;    print(&quot;\n&quot; + &quot;=&quot;*60)&#10;    print(&quot;开始训练所有专家模型&quot;)&#10;    print(&quot;=&quot;*60)&#10;    print(f&quot;专家列表: {experts}&quot;)&#10;    print(f&quot;训练轮数: {num_epochs}&quot;)&#10;    print(f&quot;设备: {config_params['device']}&quot;)&#10;    print(f&quot;保存特征嵌入: {save_embeddings}&quot;)&#10;    if save_embeddings:&#10;        print(f&quot;特征嵌入保存路径: {embeddings_dir}&quot;)&#10;    print(&quot;=&quot;*60 + &quot;\n&quot;)&#10;&#10;    # 逐个训练专家&#10;    for expert_name in experts:&#10;        print(f&quot;\n{'#'*60}&quot;)&#10;        print(f&quot;# 训练专家: {expert_name.upper()}&quot;)&#10;        print(f&quot;{'#'*60}\n&quot;)&#10;&#10;        # 检查依赖&#10;        if not check_expert_dependencies(expert_name, config_params['checkpoint_dir']):&#10;            print(f&quot;\n⚠ 跳过 {expert_name} 专家（依赖不满足）\n&quot;)&#10;            continue&#10;&#10;        try:&#10;            history = train_single_expert(expert_name, config_params, num_epochs,&#10;                                         save_embeddings=save_embeddings,&#10;                                         embeddings_dir=embeddings_dir)&#10;            results[expert_name] = history&#10;        except Exception as e:&#10;            print(f&quot;\n错误: 训练 {expert_name} 专家时出错: {str(e)}&quot;)&#10;            import traceback&#10;            traceback.print_exc()&#10;            continue&#10;&#10;    # 打印所有专家的对比结果&#10;    print(&quot;\n&quot; + &quot;=&quot;*60)&#10;    print(&quot;所有专家测试结果对比&quot;)&#10;    print(&quot;=&quot;*60)&#10;    print(f&quot;{'专家':&lt;15} {'Accuracy':&lt;12} {'F1 Score':&lt;12} {'Precision':&lt;12} {'Recall':&lt;12}&quot;)&#10;    print(&quot;-&quot;*60)&#10;&#10;    for name, history in results.items():&#10;        test = history.get('test', {})&#10;        print(f&quot;{name.upper():&lt;15} &quot;&#10;              f&quot;{test.get('acc', 0):&lt;12.4f} &quot;&#10;              f&quot;{test.get('f1', 0):&lt;12.4f} &quot;&#10;              f&quot;{test.get('precision', 0):&lt;12.4f} &quot;&#10;              f&quot;{test.get('recall', 0):&lt;12.4f}&quot;)&#10;&#10;    print(&quot;=&quot;*60 + &quot;\n&quot;)&#10;&#10;    return results&#10;&#10;&#10;def main():&#10;    &quot;&quot;&quot;主函数&quot;&quot;&quot;&#10;    parser = argparse.ArgumentParser(description='训练社交机器人检测专家模型')&#10;    parser.add_argument('--expert', type=str, default='all',&#10;                        help='要训练的专家 (des, tweets, graph, all)')&#10;    parser.add_argument('--dataset_path', type=str, default=None,&#10;                        help='数据集路径')&#10;    parser.add_argument('--checkpoint_dir', type=str, default='../../autodl-fs/checkpoints',&#10;                        help='模型保存目录')&#10;    parser.add_argument('--batch_size', type=int, default=32,&#10;                        help='批次大小')&#10;    parser.add_argument('--learning_rate', type=float, default=None,&#10;                        help='学习率 (None表示使用默认值)')&#10;    parser.add_argument('--num_epochs', type=int, default=10,&#10;                        help='训练轮数')&#10;    parser.add_argument('--device', type=str, default=None,&#10;                        help='设备 (cuda 或 cpu)')&#10;&#10;    # des和tweets Expert 参数&#10;    parser.add_argument('--bert_model', type=str, default='bert-base-uncased',&#10;                        help='BERT模型名称 (用于Description Expert)')&#10;    parser.add_argument('--roberta_model', type=str, default='distilroberta-base',&#10;                        help='RoBERTa模型名称 (用于Tweets Expert)')&#10;    parser.add_argument('--freeze_bert', action='store_true', default=True,&#10;                        help='是否冻结BERT参数 (仅训练MLP)')&#10;    parser.add_argument('--num_experts', type=int, default=4,&#10;                        help='Description Expert MoE 中的专家数量 (默认4)')&#10;    parser.add_argument('--top_k', type=int, default=2,&#10;                        help='Description Expert MoE Top-K 选择数量 (默认2)')&#10;&#10;    # Graph Expert 参数&#10;    parser.add_argument('--graph_hidden_dim', type=int, default=128,&#10;                        help='图专家隐藏层维度')&#10;    parser.add_argument('--graph_num_layers', type=int, default=2,&#10;                        help='图专家RGCN层数')&#10;    parser.add_argument('--graph_dropout', type=float, default=0.3,&#10;                        help='图专家Dropout比率')&#10;    parser.add_argument('--graph_expert_names', type=str, default='des,tweets',&#10;                        help='图专家依赖的专家列表（逗号分隔）')&#10;&#10;    # 特征嵌入保存参数&#10;    parser.add_argument('--save_embeddings', action='store_true', default=True,&#10;                        help='是否保存特征嵌入')&#10;    parser.add_argument('--embeddings_dir', type=str, default='../../autodl-fs/labeled_embedding',&#10;                        help='特征嵌入保存目录')&#10;&#10;    args = parser.parse_args()&#10;&#10;    # 确定设备&#10;    if args.device is None:&#10;        device = 'cuda' if torch.cuda.is_available() else 'cpu'&#10;    else:&#10;        device = args.device&#10;&#10;    # 解析数据集根目录，默认使用项目根目录下的 processed_data&#10;    dataset_path = args.dataset_path or str(project_root / 'processed_data')&#10;&#10;    print(f&quot;\n使用设备: {device}&quot;)&#10;&#10;    # 基础配置参数&#10;    base_config = {&#10;        'dataset_path': dataset_path,&#10;        'batch_size': args.batch_size,&#10;        'device': device,&#10;        'checkpoint_dir': args.checkpoint_dir,&#10;        # Text Expert 参数&#10;        'bert_model_name': args.bert_model,&#10;        'roberta_model_name': args.roberta_model,&#10;        'freeze_bert': args.freeze_bert,&#10;        'num_experts': args.num_experts,  # Description Expert MoE 专家数量&#10;        'top_k': args.top_k,  # Description Expert MoE Top-K 选择&#10;        # Graph Expert 参数&#10;        'hidden_dim': args.graph_hidden_dim,&#10;        'num_layers': args.graph_num_layers,&#10;        'dropout': args.graph_dropout,&#10;        'expert_names': [e.strip() for e in args.graph_expert_names.split(',')],&#10;    }&#10;&#10;    # 添加学习率 (如果指定)&#10;    if args.learning_rate is not None:&#10;        base_config['learning_rate'] = args.learning_rate&#10;&#10;    # 训练专家&#10;    if args.expert == 'all':&#10;        # 训练所有专家&#10;        results = train_all_experts(base_config, args.num_epochs,&#10;                                   save_embeddings=args.save_embeddings,&#10;                                   embeddings_dir=args.embeddings_dir)&#10;    else:&#10;        # 训练单个专家&#10;        experts_to_train = [e.strip() for e in args.expert.split(',')]&#10;        results = train_all_experts(base_config, args.num_epochs, experts=experts_to_train,&#10;                                   save_embeddings=args.save_embeddings,&#10;                                   embeddings_dir=args.embeddings_dir)&#10;&#10;    print(&quot;\n✓ 所有训练任务完成!&quot;)&#10;&#10;&#10;if __name__ == '__main__':&#10;    main()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;统一专家训练入口&#10;使用通用训练器训练所有专家模型&#10;&quot;&quot;&quot;&#10;import torch&#10;import argparse&#10;import sys&#10;from pathlib import Path&#10;import os&#10;os.environ[&quot;HF_ENDPOINT&quot;] = &quot;https://hf-mirror.com&quot;&#10;&#10;&#10;# 添加项目根目录到路径&#10;project_root = Path(__file__).parent.parent&#10;sys.path.insert(0, str(project_root))&#10;&#10;from expert_trainer import ExpertTrainer&#10;from configs.expert_configs import get_expert_config&#10;&#10;# 训练单个专家&#10;def train_single_expert(expert_name, config_params, num_epochs=10, save_embeddings=True, embeddings_dir='../../autodl-fs/labeled_embedding'):&#10;    # 获取专家配置&#10;    config = get_expert_config(expert_name, **config_params)&#10;&#10;    # 创建训练器&#10;    trainer = ExpertTrainer(config)&#10;&#10;    # 开始训练&#10;    history = trainer.train(num_epochs, save_embeddings=save_embeddings, embeddings_dir=embeddings_dir)&#10;&#10;    return history&#10;&#10;&#10;def check_expert_dependencies(expert_name, checkpoint_dir):&#10;    &quot;&quot;&quot;&#10;    检查专家的依赖是否满足&#10;&#10;    Args:&#10;        expert_name: 专家名称&#10;        checkpoint_dir: 检查点目录&#10;&#10;    Returns:&#10;        bool: 依赖是否满足&#10;    &quot;&quot;&quot;&#10;    # 定义专家依赖关系&#10;    dependencies = {&#10;        'graph': ['des', 'tweets'],  # 图专家依赖 des 和 tweets&#10;    }&#10;&#10;    if expert_name not in dependencies:&#10;        return True  # 没有依赖，直接返回True&#10;&#10;    required_experts = dependencies[expert_name]&#10;    checkpoint_dir = Path(checkpoint_dir)&#10;&#10;    print(f&quot;  检查 {expert_name} 专家的依赖...&quot;)&#10;    for dep_expert in required_experts:&#10;        checkpoint_path = checkpoint_dir / f'{dep_expert}_expert_best.pt'&#10;        if not checkpoint_path.exists():&#10;            print(f&quot;    ✗ 缺少依赖: {dep_expert} 专家未训练&quot;)&#10;            print(f&quot;    请先训练 {dep_expert} 专家&quot;)&#10;            return False&#10;        else:&#10;            print(f&quot;    ✓ {dep_expert} 专家已训练&quot;)&#10;&#10;    return True&#10;&#10;&#10;def train_all_experts(config_params, num_epochs=10, experts=None, save_embeddings=True, embeddings_dir='../../autodl-fs/labeled_embedding'):&#10;    &quot;&quot;&quot;&#10;    训练所有专家或指定的专家列表&#10;&#10;    Args:&#10;        config_params: 配置参数字典&#10;        num_epochs: 训练轮数&#10;        experts: 要训练的专家列表，None表示训练所有可用专家&#10;        save_embeddings: 是否保存特征嵌入&#10;        embeddings_dir: 特征嵌入保存目录&#10;&#10;    Returns:&#10;        dict: 所有专家的训练结果&#10;    &quot;&quot;&quot;&#10;    # 默认训练的专家列表（按依赖顺序）&#10;    if experts is None:&#10;        experts = ['des', 'tweets', 'graph']  # graph 依赖 des 和 tweets，所以放最后&#10;&#10;    results = {}&#10;&#10;    print(&quot;\n&quot; + &quot;=&quot;*60)&#10;    print(&quot;开始训练所有专家模型&quot;)&#10;    print(&quot;=&quot;*60)&#10;    print(f&quot;专家列表: {experts}&quot;)&#10;    print(f&quot;训练轮数: {num_epochs}&quot;)&#10;    print(f&quot;设备: {config_params['device']}&quot;)&#10;    print(f&quot;保存特征嵌入: {save_embeddings}&quot;)&#10;    if save_embeddings:&#10;        print(f&quot;特征嵌入保存路径: {embeddings_dir}&quot;)&#10;    print(&quot;=&quot;*60 + &quot;\n&quot;)&#10;&#10;    # 逐个训练专家&#10;    for expert_name in experts:&#10;        print(f&quot;\n{'#'*60}&quot;)&#10;        print(f&quot;# 训练专家: {expert_name.upper()}&quot;)&#10;        print(f&quot;{'#'*60}\n&quot;)&#10;&#10;        # 检查依赖&#10;        if not check_expert_dependencies(expert_name, config_params['checkpoint_dir']):&#10;            print(f&quot;\n⚠ 跳过 {expert_name} 专家（依赖不满足）\n&quot;)&#10;            continue&#10;&#10;        try:&#10;            history = train_single_expert(expert_name, config_params, num_epochs,&#10;                                         save_embeddings=save_embeddings,&#10;                                         embeddings_dir=embeddings_dir)&#10;            results[expert_name] = history&#10;        except Exception as e:&#10;            print(f&quot;\n错误: 训练 {expert_name} 专家时出错: {str(e)}&quot;)&#10;            import traceback&#10;            traceback.print_exc()&#10;            continue&#10;&#10;    # 打印所有专家的对比结果&#10;    print(&quot;\n&quot; + &quot;=&quot;*60)&#10;    print(&quot;所有专家测试结果对比&quot;)&#10;    print(&quot;=&quot;*60)&#10;    print(f&quot;{'专家':&lt;15} {'Accuracy':&lt;12} {'F1 Score':&lt;12} {'Precision':&lt;12} {'Recall':&lt;12}&quot;)&#10;    print(&quot;-&quot;*60)&#10;&#10;    for name, history in results.items():&#10;        test = history.get('test', {})&#10;        print(f&quot;{name.upper():&lt;15} &quot;&#10;              f&quot;{test.get('acc', 0):&lt;12.4f} &quot;&#10;              f&quot;{test.get('f1', 0):&lt;12.4f} &quot;&#10;              f&quot;{test.get('precision', 0):&lt;12.4f} &quot;&#10;              f&quot;{test.get('recall', 0):&lt;12.4f}&quot;)&#10;&#10;    print(&quot;=&quot;*60 + &quot;\n&quot;)&#10;&#10;    return results&#10;&#10;&#10;def main():&#10;    &quot;&quot;&quot;主函数&quot;&quot;&quot;&#10;    parser = argparse.ArgumentParser(description='训练社交机器人检测专家模型')&#10;    parser.add_argument('--expert', type=str, default='all',&#10;                        help='要训练的专家 (des, tweets, graph, all)')&#10;    parser.add_argument('--dataset_path', type=str, default=None,&#10;                        help='数据集路径')&#10;    parser.add_argument('--checkpoint_dir', type=str, default='../../autodl-fs/model',&#10;                        help='模型保存目录')&#10;    parser.add_argument('--batch_size', type=int, default=32,&#10;                        help='批次大小')&#10;    parser.add_argument('--learning_rate', type=float, default=None,&#10;                        help='学习率 (None表示使用默认值)')&#10;    parser.add_argument('--num_epochs', type=int, default=10,&#10;                        help='训练轮数')&#10;    parser.add_argument('--device', type=str, default=None,&#10;                        help='设备 (cuda 或 cpu)')&#10;&#10;    # des和tweets Expert 参数&#10;    parser.add_argument('--bert_model', type=str, default='bert-base-uncased',&#10;                        help='BERT模型名称 (用于Description Expert)')&#10;    parser.add_argument('--roberta_model', type=str, default='distilroberta-base',&#10;                        help='RoBERTa模型名称 (用于Tweets Expert)')&#10;    parser.add_argument('--freeze_bert', action='store_true', default=True,&#10;                        help='是否冻结BERT参数 (仅训练MLP)')&#10;    parser.add_argument('--num_experts', type=int, default=4,&#10;                        help='Description Expert MoE 中的专家数量 (默认4)')&#10;    parser.add_argument('--top_k', type=int, default=2,&#10;                        help='Description Expert MoE Top-K 选择数量 (默认2)')&#10;&#10;    # Graph Expert 参数&#10;    parser.add_argument('--graph_hidden_dim', type=int, default=128,&#10;                        help='图专家隐藏层维度')&#10;    parser.add_argument('--graph_num_layers', type=int, default=2,&#10;                        help='图专家RGCN层数')&#10;    parser.add_argument('--graph_dropout', type=float, default=0.3,&#10;                        help='图专家Dropout比率')&#10;    parser.add_argument('--graph_expert_names', type=str, default='des,tweets',&#10;                        help='图专家依赖的专家列表（逗号分隔）')&#10;&#10;    # 特征嵌入保存参数&#10;    parser.add_argument('--save_embeddings', action='store_true', default=True,&#10;                        help='是否保存特征嵌入')&#10;    parser.add_argument('--embeddings_dir', type=str, default='../../autodl-fs/labeled_embedding',&#10;                        help='特征嵌入保存目录')&#10;&#10;    args = parser.parse_args()&#10;&#10;    # 确定设备&#10;    if args.device is None:&#10;        device = 'cuda' if torch.cuda.is_available() else 'cpu'&#10;    else:&#10;        device = args.device&#10;&#10;    # 解析数据集根目录，默认使用项目根目录下的 processed_data&#10;    dataset_path = args.dataset_path or str(project_root / 'processed_data')&#10;&#10;    print(f&quot;\n使用设备: {device}&quot;)&#10;&#10;    # 基础配置参数&#10;    base_config = {&#10;        'dataset_path': dataset_path,&#10;        'batch_size': args.batch_size,&#10;        'device': device,&#10;        'checkpoint_dir': args.checkpoint_dir,&#10;        # Text Expert 参数&#10;        'bert_model_name': args.bert_model,&#10;        'roberta_model_name': args.roberta_model,&#10;        'freeze_bert': args.freeze_bert,&#10;        'num_experts': args.num_experts,  # Description Expert MoE 专家数量&#10;        'top_k': args.top_k,  # Description Expert MoE Top-K 选择&#10;        # Graph Expert 参数&#10;        'hidden_dim': args.graph_hidden_dim,&#10;        'num_layers': args.graph_num_layers,&#10;        'dropout': args.graph_dropout,&#10;        'expert_names': [e.strip() for e in args.graph_expert_names.split(',')],&#10;    }&#10;&#10;    # 添加学习率 (如果指定)&#10;    if args.learning_rate is not None:&#10;        base_config['learning_rate'] = args.learning_rate&#10;&#10;    # 训练专家&#10;    if args.expert == 'all':&#10;        # 训练所有专家&#10;        results = train_all_experts(base_config, args.num_epochs,&#10;                                   save_embeddings=args.save_embeddings,&#10;                                   embeddings_dir=args.embeddings_dir)&#10;    else:&#10;        # 训练单个专家&#10;        experts_to_train = [e.strip() for e in args.expert.split(',')]&#10;        results = train_all_experts(base_config, args.num_epochs, experts=experts_to_train,&#10;                                   save_embeddings=args.save_embeddings,&#10;                                   embeddings_dir=args.embeddings_dir)&#10;&#10;    print(&quot;\n✓ 所有训练任务完成!&quot;)&#10;&#10;&#10;if __name__ == '__main__':&#10;    main()&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>